{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.092371481123546 33.86995503986883 65.31070730244805 137.3936494261715\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "f = gzip.open('renttherunway_final_data.json.gz', 'r')\n",
    "\n",
    "ratings = []\n",
    "ages = []\n",
    "weights = []\n",
    "heights = []\n",
    "\n",
    "for l in f:\n",
    "    try:\n",
    "        x = eval(l)\n",
    "        \n",
    "        # Data cleaning: convert height, weight, and rating to integer types\n",
    "        x['rating'] = int(x['rating'])\n",
    "        ratings.append(x['rating'])\n",
    "\n",
    "        if 'age' in x:\n",
    "            x['age'] = int(x['age'])\n",
    "            ages.append(x['age'])\n",
    "        if 'weight' in x:\n",
    "            x['weight'] = int(x['weight'][:-3])\n",
    "            weights.append(x['weight'])\n",
    "        if 'height' in x:\n",
    "            feet = int(x['height'][0])\n",
    "            inches = int(x['height'][-3:-1].strip(\" \"))\n",
    "            x['height'] = (feet * 12) + inches\n",
    "            heights.append(x['height'])\n",
    "\n",
    "        data.append(x)\n",
    "    except NameError:\n",
    "        continue\n",
    "\n",
    "average_rating = sum(ratings) / len(ratings)\n",
    "\n",
    "average_age = sum(ages) / len(ages)\n",
    "average_weight = sum(weights) / len(weights)\n",
    "average_height = sum(heights) / len(heights)\n",
    "\n",
    "print(average_rating, average_age, average_height, average_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit': 'fit',\n",
       " 'user_id': '420272',\n",
       " 'bust size': '34d',\n",
       " 'item_id': '2260466',\n",
       " 'weight': 137,\n",
       " 'rating': 10,\n",
       " 'rented for': 'vacation',\n",
       " 'review_text': \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
       " 'body type': 'hourglass',\n",
       " 'review_summary': 'So many compliments!',\n",
       " 'category': 'romper',\n",
       " 'height': 68,\n",
       " 'size': 14,\n",
       " 'age': 28,\n",
       " 'review_date': 'April 20, 2016'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert EDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression analysis\n",
    "\n",
    "def train_and_predict_model(feature_fn):\n",
    "  # Training/test/validation sets - 70% train, 15% valid, 15% test\n",
    "  X = [feature_fn(d) for d in data]\n",
    "  y = [d['rating'] for d in data]\n",
    "\n",
    "  Xtrain = X[:7 * len(X) // 10]\n",
    "  ytrain = y[:7 * len(y) // 10]\n",
    "  Xvalid = X[7 * len(X) // 10: int(8.5 * len(X) // 10)]\n",
    "  yvalid = y[7 * len(y) // 10: int(8.5 * len(y) // 10)]\n",
    "  Xtest = X[int(8.5 * len(X) // 10):]\n",
    "  ytest = y[int(8.5 * len(y) // 10):]\n",
    "\n",
    "  # Model creation\n",
    "  model = linear_model.LinearRegression(fit_intercept=False)\n",
    "  model.fit(Xtrain, np.matrix(ytrain).T)\n",
    "\n",
    "  # Model validation\n",
    "  ans = model.predict(Xvalid)\n",
    "  actual = ans.T[0]\n",
    "\n",
    "  mse = MSE(yvalid, actual)\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline rating model - predict the average rating for every prediction\n",
    "def feature_base(datum):\n",
    "  return [1, average_rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0101764219048124"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_base = train_and_predict_model(feature_base)\n",
    "mse_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #1: use the number of exclamation marks in review text to predict rating\n",
    "def feature_1(datum):\n",
    "  if 'review_text' in datum:\n",
    "    return [1, datum['review_text'].count('!')]\n",
    "  return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9444446124699253"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_1 = train_and_predict_model(feature_1)\n",
    "mse_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #2: use user attributes such as height, weight, and age along with item size to predict ratings of item \n",
    "def feature_2(datum):\n",
    "  weight = datum['weight'] if 'weight' in datum else average_weight\n",
    "  height = datum['height'] if 'height' in datum else average_height\n",
    "  age = datum['age'] if 'age' in datum else average_age\n",
    "  size = datum['size']\n",
    "  return [1, weight, height, age, size]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0062008791177117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_2 = train_and_predict_model(feature_2)\n",
    "mse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #3: use user attributes along with review text exclamations\n",
    "def feature_3(datum):\n",
    "  weight = datum['weight'] if 'weight' in datum else average_weight\n",
    "  height = datum['height'] if 'height' in datum else average_height\n",
    "  age = datum['age'] if 'age' in datum else average_age\n",
    "  size = datum['size']\n",
    "  excl_count = datum['review_text'].count('!') if 'review_text' in datum else 0\n",
    "  return [1, weight, height, age, size, excl_count]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942181367154325"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_3 = train_and_predict_model(feature_3)\n",
    "mse_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0101764219048124, 1.9444446124699253, 2.0062008791177117, 1.942181367154325]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses_linreg = [mse_base, mse_1, mse_2, mse_3]\n",
    "mses_linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction prediction\n",
    "dataTrain = data[:7*len(data) // 10]\n",
    "dataValid = data[7*len(data) // 10 : int(8.5 * len(data) // 10)]\n",
    "dataTest = data[int(8.5 * len(data) // 10):]\n",
    "\n",
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "trainingRatings = []\n",
    "\n",
    "for d in dataTrain:\n",
    "    user, item, rating = d['user_id'], d['item_id'], d['rating']\n",
    "    \n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    ratingDict[(user, item)] = rating\n",
    "    \n",
    "    trainingRatings.append(rating)\n",
    "    \n",
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    rs = [ratingDict[(u,i)] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)\n",
    "    \n",
    "ratingMean = sum(trainingRatings) / len(trainingRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    \n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2100620042373786"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    sims = []\n",
    "    \n",
    "    for d in reviewsPerUser[user]:\n",
    "        j = d['item_id']\n",
    "        \n",
    "        if j == item:\n",
    "            continue\n",
    "        \n",
    "        ratings.append(ratingDict[(user, j)] - itemAverages[j])\n",
    "        sims.append(Jaccard(usersPerItem[item], usersPerItem[j]))\n",
    "    \n",
    "    if sum(sims) > 0:\n",
    "        weightedRatings = [(x * y) for x, y in zip(ratings, sims)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(sims)\n",
    "    else:\n",
    "        return ratingMean\n",
    "\n",
    "simPredictions = [predictRating(d['user_id'], d['item_id']) for d in dataValid]\n",
    "labels = [d['rating'] for d in dataValid]\n",
    "\n",
    "mse = MSE(simPredictions, labels)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta gamma fuck shit\n",
    "dataTrain = data[:7*len(data) // 10]\n",
    "dataValid = data[7*len(data) // 10 :]\n",
    "\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataTrain:\n",
    "    u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "    \n",
    "    ratingsPerUser[u].append((i,r))\n",
    "    ratingsPerItem[i].append((u,r))\n",
    "\n",
    "trainRatings = [d['rating'] for d in dataTrain]\n",
    "globalAverage = sum(trainRatings) * 1.0 / len(trainRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in ratingsPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for i in ratingsPerItem:\n",
    "    betaI[i] = 0\n",
    "\n",
    "alpha = globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(lamb):\n",
    "    newAlpha = 0\n",
    "    for d in dataTrain:\n",
    "        u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "        newAlpha += r - (betaU[u] + betaI[i])\n",
    "    alpha = newAlpha / len(dataTrain)\n",
    "    for u in ratingsPerUser:\n",
    "        newBetaU = 0\n",
    "        for i,r in ratingsPerUser[u]:\n",
    "            newBetaU += r - (alpha + betaI[i])\n",
    "        betaU[u] = newBetaU / (lamb + len(ratingsPerUser[u]))\n",
    "    for i in ratingsPerItem:\n",
    "        newBetaI = 0\n",
    "        for u,r in ratingsPerItem[i]:\n",
    "            newBetaI += r - (alpha + betaU[u])\n",
    "        betaI[i] = newBetaI / (lamb + len(ratingsPerItem[i]))\n",
    "    mse = 0\n",
    "    for d in dataTrain:\n",
    "        u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "        prediction = alpha + betaU[u] + betaI[i]\n",
    "        mse += (r - prediction)**2\n",
    "    regularizer = 0\n",
    "    for u in betaU:\n",
    "        regularizer += betaU[u]**2\n",
    "    for i in betaI:\n",
    "        regularizer += betaI[i]**2\n",
    "    mse /= len(dataTrain)\n",
    "    return mse, mse + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse,objective = iterate(1)\n",
    "newMSE,newObjective = iterate(1)\n",
    "iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 3 iterations = 23311.166874083257\n",
      "MSE after 3 iterations = 1.5622401007624203\n",
      "Objective after 4 iterations = 22939.439119740124\n",
      "MSE after 4 iterations = 1.5635516616259384\n",
      "Objective after 5 iterations = 22920.76285641952\n",
      "MSE after 5 iterations = 1.5635794588707819\n",
      "Objective after 6 iterations = 22918.607186913378\n",
      "MSE after 6 iterations = 1.563521470547877\n",
      "Objective after 7 iterations = 22918.82083238304\n",
      "MSE after 7 iterations = 1.5634710974168786\n",
      "Objective after 8 iterations = 22919.77964959742\n",
      "MSE after 8 iterations = 1.5634322438593167\n",
      "Objective after 9 iterations = 22921.007971392628\n",
      "MSE after 9 iterations = 1.563402567564283\n",
      "Objective after 10 iterations = 22922.28293229403\n",
      "MSE after 10 iterations = 1.5633798023686516\n"
     ]
    }
   ],
   "source": [
    "while iterations < 10 or objective - newObjective > 0.0001:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(10)\n",
    "    iterations += 1\n",
    "    print(\"Objective after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newObjective))\n",
    "    print(\"MSE after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = 1.9081931152299059\n"
     ]
    }
   ],
   "source": [
    "validMSE = 0\n",
    "for d in dataValid:\n",
    "    u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if i in betaI:\n",
    "        bi = betaI[i]\n",
    "    prediction = alpha + bu + bi\n",
    "    validMSE += (r - prediction) ** 2\n",
    "\n",
    "validMSE /= len(dataValid)\n",
    "print(\"Validation MSE = \" + str(validMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = [1.90818754]\n"
     ]
    }
   ],
   "source": [
    "# Proposed model: combines interaction data and feature data\n",
    "# Idea: if the user and item have never been seen before in the dataset,\n",
    "# make a prediction using the best linear regression model we examined.\n",
    "\n",
    "feature_fn = feature_3\n",
    "\n",
    "# Train the model\n",
    "X = [feature_fn(d) for d in dataTrain]\n",
    "y = [d['rating'] for d in dataTrain]\n",
    "\n",
    "# Model creation\n",
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(X, np.matrix(y).T)\n",
    "\n",
    "c = 0\n",
    "\n",
    "# Combine with interaction data\n",
    "proposedValidMSE = 0\n",
    "for d in dataValid:\n",
    "    u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if i in betaI:\n",
    "        bi = betaI[i]\n",
    "    \n",
    "    if not bi and not bu:\n",
    "        c += 1\n",
    "        prediction = model.predict([feature_fn(d)]).T[0]\n",
    "    else:\n",
    "        prediction = alpha + bu + bi\n",
    "    proposedValidMSE += (r - prediction) ** 2\n",
    "\n",
    "proposedValidMSE /= len(dataValid)\n",
    "print(\"Validation MSE = \" + str(proposedValidMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposedValidMSE[0] - validMSE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
