{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'fit': 141995, 'small': 25776, 'large': 24691})\n",
      "fit\n",
      "9.092371481123546 33.86995503986883 65.31070730244805 137.3936494261715\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "f = gzip.open('renttherunway_final_data.json.gz', 'r')\n",
    "\n",
    "ratings = []\n",
    "ages = []\n",
    "weights = []\n",
    "heights = []\n",
    "\n",
    "fits = defaultdict(int)\n",
    "fit_to_float = {\"small\": 0, \"fit\": 1, \"large\": 2}\n",
    "\n",
    "for l in f:\n",
    "    try:\n",
    "        x = eval(l)\n",
    "\n",
    "        fits[x['fit']] += 1\n",
    "        \n",
    "        # Data cleaning: convert height, weight, and rating to integer types\n",
    "        x['rating'] = int(x['rating'])\n",
    "        ratings.append(x['rating'])\n",
    "\n",
    "        if 'age' in x:\n",
    "            x['age'] = int(x['age'])\n",
    "            ages.append(x['age'])\n",
    "        if 'weight' in x:\n",
    "            x['weight'] = int(x['weight'][:-3])\n",
    "            weights.append(x['weight'])\n",
    "        if 'height' in x:\n",
    "            feet = int(x['height'][0])\n",
    "            inches = int(x['height'][-3:-1].strip(\" \"))\n",
    "            x['height'] = (feet * 12) + inches\n",
    "            heights.append(x['height'])\n",
    "\n",
    "        data.append(x)\n",
    "    except NameError:\n",
    "        continue\n",
    "\n",
    "average_rating = sum(ratings) / len(ratings)\n",
    "\n",
    "average_age = sum(ages) / len(ages)\n",
    "average_weight = sum(weights) / len(weights)\n",
    "average_height = sum(heights) / len(heights)\n",
    "\n",
    "# find most common fit\n",
    "print(fits)\n",
    "highest_fit_freq = max(fits.values())\n",
    "most_common_fit = None\n",
    "for k, v in fits.items():\n",
    "    if v == highest_fit_freq:\n",
    "        most_common_fit = k\n",
    "        break\n",
    "print(most_common_fit)\n",
    "print(average_rating, average_age, average_height, average_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit': 'fit',\n",
       " 'user_id': '420272',\n",
       " 'bust size': '34d',\n",
       " 'item_id': '2260466',\n",
       " 'weight': 137,\n",
       " 'rating': 10,\n",
       " 'rented for': 'vacation',\n",
       " 'review_text': \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
       " 'body type': 'hourglass',\n",
       " 'review_summary': 'So many compliments!',\n",
       " 'category': 'romper',\n",
       " 'height': 68,\n",
       " 'size': 14,\n",
       " 'age': 28,\n",
       " 'review_date': 'April 20, 2016'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192462"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression analysis\n",
    "\n",
    "def train_and_predict_model(feature_fn):\n",
    "  # Training/test/validation sets - 70% train, 15% valid, 15% test\n",
    "  X = [feature_fn(d) for d in data]\n",
    "  y = [d['rating'] for d in data]\n",
    "\n",
    "  Xtrain = X[:7 * len(X) // 10]\n",
    "  ytrain = y[:7 * len(y) // 10]\n",
    "  Xvalid = X[7 * len(X) // 10: int(8.5 * len(X) // 10)]\n",
    "  yvalid = y[7 * len(y) // 10: int(8.5 * len(y) // 10)]\n",
    "  Xtest = X[int(8.5 * len(X) // 10):]\n",
    "  ytest = y[int(8.5 * len(y) // 10):]\n",
    "\n",
    "  # Model creation\n",
    "  model = linear_model.LinearRegression(fit_intercept=False)\n",
    "  model.fit(Xtrain, np.matrix(ytrain).T)\n",
    "\n",
    "  # Model validation\n",
    "  ans = model.predict(Xvalid)\n",
    "  actual = ans.T[0]\n",
    "\n",
    "  mse = MSE(yvalid, actual)\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline rating model - predict the average rating for every prediction\n",
    "def feature_base(datum):\n",
    "  return [1, average_rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0101764219048124"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_base = train_and_predict_model(feature_base)\n",
    "mse_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #1: use the number of exclamation marks in review text to predict rating\n",
    "def feature_1(datum):\n",
    "  if 'review_text' in datum:\n",
    "    return [1, datum['review_text'].count('!')]\n",
    "  return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9444446124699253"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_1 = train_and_predict_model(feature_1)\n",
    "mse_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #2: use user attributes such as height, weight, and age along with item size to predict ratings of item \n",
    "def feature_2(datum):\n",
    "  weight = datum['weight'] if 'weight' in datum else average_weight\n",
    "  height = datum['height'] if 'height' in datum else average_height\n",
    "  age = datum['age'] if 'age' in datum else average_age\n",
    "  size = datum['size']\n",
    "  return [1, weight, height, age, size]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0062008791177117"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_2 = train_and_predict_model(feature_2)\n",
    "mse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature enginering #3: use user attributes along with review text exclamations\n",
    "def feature_3(datum):\n",
    "  weight = datum['weight'] if 'weight' in datum else average_weight\n",
    "  height = datum['height'] if 'height' in datum else average_height\n",
    "  age = datum['age'] if 'age' in datum else average_age\n",
    "  size = datum['size']\n",
    "  excl_count = datum['review_text'].count('!') if 'review_text' in datum else 0\n",
    "  return [1, weight, height, age, size, excl_count]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.942181367154325"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_3 = train_and_predict_model(feature_3)\n",
    "mse_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0101764219048124, 1.9444446124699253, 2.0062008791177117, 1.942181367154325]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses_linreg = [mse_base, mse_1, mse_2, mse_3]\n",
    "mses_linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction prediction\n",
    "dataTrain = data[:7*len(data) // 10]\n",
    "dataValid = data[7*len(data) // 10 : int(8.5 * len(data) // 10)]\n",
    "dataTest = data[int(8.5 * len(data) // 10):]\n",
    "\n",
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "trainingRatings = []\n",
    "\n",
    "for d in dataTrain:\n",
    "    user, item, rating = d['user_id'], d['item_id'], d['rating']\n",
    "    \n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    ratingDict[(user, item)] = rating\n",
    "    \n",
    "    trainingRatings.append(rating)\n",
    "    \n",
    "userAverages = {}\n",
    "itemAverages = {}\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    rs = [ratingDict[(u,i)] for i in itemsPerUser[u]]\n",
    "    userAverages[u] = sum(rs) / len(rs)\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)\n",
    "    \n",
    "ratingMean = sum(trainingRatings) / len(trainingRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    \n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2100620042373786"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    sims = []\n",
    "    \n",
    "    for d in reviewsPerUser[user]:\n",
    "        j = d['item_id']\n",
    "        \n",
    "        if j == item:\n",
    "            continue\n",
    "        \n",
    "        ratings.append(ratingDict[(user, j)] - itemAverages[j])\n",
    "        sims.append(Jaccard(usersPerItem[item], usersPerItem[j]))\n",
    "    \n",
    "    if sum(sims) > 0:\n",
    "        weightedRatings = [(x * y) for x, y in zip(ratings, sims)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(sims)\n",
    "    else:\n",
    "        return ratingMean\n",
    "\n",
    "simPredictions = [predictRating(d['user_id'], d['item_id']) for d in dataValid]\n",
    "labels = [d['rating'] for d in dataValid]\n",
    "\n",
    "mse = MSE(simPredictions, labels)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta gamma fuck shit\n",
    "dataTrain = data[:7*len(data) // 10]\n",
    "dataValid = data[7*len(data) // 10 :]\n",
    "\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataTrain:\n",
    "    u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "    \n",
    "    ratingsPerUser[u].append((i,r))\n",
    "    ratingsPerItem[i].append((u,r))\n",
    "\n",
    "trainRatings = [d['rating'] for d in dataTrain]\n",
    "globalAverage = sum(trainRatings) * 1.0 / len(trainRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaU = {}\n",
    "betaI = {}\n",
    "for u in ratingsPerUser:\n",
    "    betaU[u] = 0\n",
    "\n",
    "for i in ratingsPerItem:\n",
    "    betaI[i] = 0\n",
    "\n",
    "alpha = globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(lamb):\n",
    "    newAlpha = 0\n",
    "    for d in dataTrain:\n",
    "        u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "        newAlpha += r - (betaU[u] + betaI[i])\n",
    "    alpha = newAlpha / len(dataTrain)\n",
    "    for u in ratingsPerUser:\n",
    "        newBetaU = 0\n",
    "        for i,r in ratingsPerUser[u]:\n",
    "            newBetaU += r - (alpha + betaI[i])\n",
    "        betaU[u] = newBetaU / (lamb + len(ratingsPerUser[u]))\n",
    "    for i in ratingsPerItem:\n",
    "        newBetaI = 0\n",
    "        for u,r in ratingsPerItem[i]:\n",
    "            newBetaI += r - (alpha + betaU[u])\n",
    "        betaI[i] = newBetaI / (lamb + len(ratingsPerItem[i]))\n",
    "    mse = 0\n",
    "    for d in dataTrain:\n",
    "        u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "        prediction = alpha + betaU[u] + betaI[i]\n",
    "        mse += (r - prediction)**2\n",
    "    regularizer = 0\n",
    "    for u in betaU:\n",
    "        regularizer += betaU[u]**2\n",
    "    for i in betaI:\n",
    "        regularizer += betaI[i]**2\n",
    "    mse /= len(dataTrain)\n",
    "    return mse, mse + lamb*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse,objective = iterate(1)\n",
    "newMSE,newObjective = iterate(1)\n",
    "iterations = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after 3 iterations = 25738.34143408642\n",
      "MSE after 3 iterations = 1.5042454444325741\n",
      "Objective after 4 iterations = 25321.19478518339\n",
      "MSE after 4 iterations = 1.5059605277936217\n",
      "Objective after 5 iterations = 25294.731834989765\n",
      "MSE after 5 iterations = 1.5060566535120947\n",
      "Objective after 6 iterations = 25290.436454075556\n",
      "MSE after 6 iterations = 1.5060178879246224\n",
      "Objective after 7 iterations = 25289.323357079003\n",
      "MSE after 7 iterations = 1.505976000547445\n",
      "Objective after 8 iterations = 25289.25698290257\n",
      "MSE after 8 iterations = 1.5059415284577338\n",
      "Objective after 9 iterations = 25289.693676870473\n",
      "MSE after 9 iterations = 1.5059140695752733\n",
      "Objective after 10 iterations = 25290.378147989442\n",
      "MSE after 10 iterations = 1.5058922528183711\n"
     ]
    }
   ],
   "source": [
    "while iterations < 10 or objective - newObjective > 0.0001:\n",
    "    mse, objective = newMSE, newObjective\n",
    "    newMSE, newObjective = iterate(8)\n",
    "    iterations += 1\n",
    "    print(\"Objective after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newObjective))\n",
    "    print(\"MSE after \"\n",
    "        + str(iterations) + \" iterations = \" + str(newMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = 1.9075954316581416\n"
     ]
    }
   ],
   "source": [
    "validMSE = 0\n",
    "for d in dataValid:\n",
    "    u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "\n",
    "    bu = 0\n",
    "    bi = 0\n",
    "    if u in betaU:\n",
    "        bu = betaU[u]\n",
    "    if i in betaI:\n",
    "        bi = betaI[i]\n",
    "    prediction = alpha + bu + bi\n",
    "    validMSE += (r - prediction) ** 2\n",
    "\n",
    "validMSE /= len(dataValid)\n",
    "print(\"Validation MSE = \" + str(validMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE = [1.90758986]\n",
      "Test MSE = [1.92722239]\n"
     ]
    }
   ],
   "source": [
    "# Proposed model: combines interaction data and feature data\n",
    "# Idea: if the user and item have never been seen before in the dataset,\n",
    "# make a prediction using the best linear regression model we examined.\n",
    "\n",
    "feature_fn = feature_3\n",
    "\n",
    "def find_bias_mse(dataset):\n",
    "    mse = 0\n",
    "    for d in dataset:\n",
    "        u, i, r = d['user_id'], d['item_id'], d['rating']\n",
    "\n",
    "        bu = 0\n",
    "        bi = 0\n",
    "        if u in betaU:\n",
    "            bu = betaU[u]\n",
    "        if i in betaI:\n",
    "            bi = betaI[i]\n",
    "        \n",
    "        if not bi and not bu:\n",
    "            prediction = model.predict([feature_fn(d)]).T[0]\n",
    "        else:\n",
    "            prediction = alpha + bu + bi\n",
    "        mse += (r - prediction) ** 2\n",
    "    mse /= len(dataset)\n",
    "    return mse\n",
    "\n",
    "# Train the model\n",
    "X = [feature_fn(d) for d in dataTrain]\n",
    "y = [d['rating'] for d in dataTrain]\n",
    "\n",
    "# Model creation\n",
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(X, np.matrix(y).T)\n",
    "\n",
    "\n",
    "\n",
    "# Combine with interaction data and run on validation and test sets\n",
    "validationMSE = find_bias_mse(dataValid)\n",
    "testMSE = find_bias_mse(dataTest)\n",
    "\n",
    "print(\"Validation MSE = \" + str(validationMSE))\n",
    "print(\"Test MSE = \" + str(testMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit': 'fit',\n",
       " 'user_id': '420272',\n",
       " 'bust size': '34d',\n",
       " 'item_id': '2260466',\n",
       " 'weight': 137,\n",
       " 'rating': 10,\n",
       " 'rented for': 'vacation',\n",
       " 'review_text': \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\",\n",
       " 'body type': 'hourglass',\n",
       " 'review_summary': 'So many compliments!',\n",
       " 'category': 'romper',\n",
       " 'height': 68,\n",
       " 'size': 14,\n",
       " 'age': 28,\n",
       " 'review_date': 'April 20, 2016'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    correct = predictions == y\n",
    "    return sum(correct) / len(correct)\n",
    "\n",
    "def train_and_predict_model(feature_fn):\n",
    "  # Training/test/validation sets - 70% train, 15% valid, 15% test\n",
    "  X = [feature_fn(d) for d in data]\n",
    "  y = [fit_to_float[d['fit']] for d in data]\n",
    "\n",
    "  Xtrain = X[:7 * len(X) // 10]\n",
    "  ytrain = y[:7 * len(y) // 10]\n",
    "  Xvalid = X[7 * len(X) // 10: int(8.5 * len(X) // 10)]\n",
    "  yvalid = y[7 * len(y) // 10: int(8.5 * len(y) // 10)]\n",
    "  Xtest = X[int(8.5 * len(X) // 10):]\n",
    "  ytest = y[int(8.5 * len(y) // 10):]\n",
    "\n",
    "  # Model creation\n",
    "  model = linear_model.LogisticRegression(class_weight='balanced')\n",
    "  model.fit(Xtrain, ytrain)\n",
    "\n",
    "  # Model validation\n",
    "  predictions = model.predict(Xvalid)\n",
    "  print(set(predictions))\n",
    "  return accuracy(predictions, yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prediction: guess the most common fit seen every time\n",
    "def feature_base(datum):\n",
    "  return [1, fit_to_float[most_common_fit]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13110949461359936"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_base = train_and_predict_model(feature_base)\n",
    "acc_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_1(datum):\n",
    "  if 'review_text' in datum:\n",
    "    return [1, datum['review_text'].count('!')]\n",
    "  return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4504832172919048"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_1 = train_and_predict_model(feature_1)\n",
    "acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.255533617375039"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_2 = train_and_predict_model(feature_2)\n",
    "acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27396168900897155"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_3 = train_and_predict_model(feature_3)\n",
    "acc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words approach for reviews\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "        \n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:100]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature_bow(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530979658639233"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [feature_bow(d) for d in data]\n",
    "y = [fit_to_float[d['fit']] for d in data]\n",
    "\n",
    "Xtrain = X[:8*len(X)//10]\n",
    "ytrain = y[:8*len(y)//10]\n",
    "Xvalid = X[8*len(X)//10:]\n",
    "yvalid = y[8*len(y)//10:]\n",
    "\n",
    "# Logistic regressor\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(Xvalid)\n",
    "\n",
    "numCorrect = sum([predictions[i] == yvalid[i] for i in range(len(yvalid))])\n",
    "acc_bow = numCorrect / len(yvalid)\n",
    "acc_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
